{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63a0155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.12.8)\n",
      "Requirement already satisfied: llama-index-readers-obsidian in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: ipython in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (8.15.0)\n",
      "Requirement already satisfied: huggingface-hub in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: openai in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.58.1)\n",
      "Requirement already satisfied: pyyaml in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: llama-index-llms-ollama in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.5.0)\n",
      "Requirement already satisfied: llama-index-embeddings-ollama in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.5.0)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.4.0)\n",
      "Requirement already satisfied: ipykernel in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (6.25.0)\n",
      "Collecting pyvis (from -r requirements.txt (line 13))\n",
      "  Obtaining dependency information for pyvis from https://files.pythonhosted.org/packages/ab/4b/e37e4e5d5ee1179694917b445768bdbfb084f5a59ecd38089d3413d4c70f/pyvis-0.3.2-py3-none-any.whl.metadata\n",
      "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.8 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.12.8)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.3.1)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.3.12)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.3.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.3.0)\n",
      "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: nltk>3.8.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index->-r requirements.txt (line 1)) (3.9.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipywidgets->-r requirements.txt (line 3)) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipywidgets->-r requirements.txt (line 3)) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipywidgets->-r requirements.txt (line 3)) (3.0.5)\n",
      "Requirement already satisfied: backcall in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipython->-r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: filelock in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from huggingface-hub->-r requirements.txt (line 6)) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from huggingface-hub->-r requirements.txt (line 6)) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from huggingface-hub->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: requests in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from huggingface-hub->-r requirements.txt (line 6)) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from huggingface-hub->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 7)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 7)) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 7)) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 7)) (2.10.4)\n",
      "Requirement already satisfied: sniffio in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: ollama>=0.4.3 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-llms-ollama->-r requirements.txt (line 9)) (0.4.4)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (3.3.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (5.3.0)\n",
      "Requirement already satisfied: nest-asyncio in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from ipykernel->-r requirements.txt (line 12)) (6.3.2)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from pyvis->-r requirements.txt (line 13)) (3.1.4)\n",
      "Collecting jsonpickle>=1.4.1 (from pyvis->-r requirements.txt (line 13))\n",
      "  Obtaining dependency information for jsonpickle>=1.4.1 from https://files.pythonhosted.org/packages/86/7c/c06580145924f60342f669f6e71905f838083d00e4b141172a75d22a23fc/jsonpickle-4.0.1-py3-none-any.whl.metadata\n",
      "  Downloading jsonpickle-4.0.1-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: networkx>=1.11 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from pyvis->-r requirements.txt (line 13)) (3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: certifi in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 7)) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 7)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 7)) (0.14.0)\n",
      "Requirement already satisfied: aiohttp in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from huggingface-hub->-r requirements.txt (line 6)) (3.11.11)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from jedi>=0.16->ipython->-r requirements.txt (line 5)) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from jinja2>=2.9.6->pyvis->-r requirements.txt (line 13)) (2.1.1)\n",
      "Requirement already satisfied: entrypoints in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 12)) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 12)) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (2.0.36)\n",
      "Requirement already satisfied: dataclasses-json in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (1.2.15)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: numpy in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (10.4.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: llama-cloud>=0.1.5 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->-r requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 1)) (4.12.3)\n",
      "Requirement already satisfied: pandas in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 1)) (5.1.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 1)) (0.0.26)\n",
      "Requirement already satisfied: llama-parse>=0.5.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->-r requirements.txt (line 1)) (0.5.18)\n",
      "Requirement already satisfied: click in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from nltk>3.8.1->llama-index->-r requirements.txt (line 1)) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from nltk>3.8.1->llama-index->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from nltk>3.8.1->llama-index->-r requirements.txt (line 1)) (2022.7.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from pexpect>4.3->ipython->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->-r requirements.txt (line 5)) (0.2.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai->-r requirements.txt (line 7)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->-r requirements.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub->-r requirements.txt (line 6)) (1.26.16)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: scipy in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (1.11.1)\n",
      "Requirement already satisfied: executing in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from stack-data->ipython->-r requirements.txt (line 5)) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from stack-data->ipython->-r requirements.txt (line 5)) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from stack-data->ipython->-r requirements.txt (line 5)) (0.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from aiohttp->huggingface-hub->-r requirements.txt (line 6)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from aiohttp->huggingface-hub->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from aiohttp->huggingface-hub->-r requirements.txt (line 6)) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from aiohttp->huggingface-hub->-r requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from aiohttp->huggingface-hub->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from aiohttp->huggingface-hub->-r requirements.txt (line 6)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from aiohttp->huggingface-hub->-r requirements.txt (line 6)) (1.18.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 1)) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (2.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (0.4.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.8->llama-index->-r requirements.txt (line 1)) (3.23.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/arthursarazin/anaconda3/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface->-r requirements.txt (line 11)) (2.2.0)\n",
      "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpickle-4.0.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jsonpickle, pyvis\n",
      "Successfully installed jsonpickle-4.0.1 pyvis-0.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614fc4e2-0e04-49b0-bf7c-05efee48933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.obsidian import ObsidianReader\n",
    "from llama_index.core.memory.chat_memory_buffer import MessageRole\n",
    "from llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex, VectorStoreIndex\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import Document, PropertyGraphIndex\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore\n",
    "from llama_index.core.vector_stores import SimpleVectorStore\n",
    "from llama_index.core import Settings\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import os\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "import logging\n",
    "import sys\n",
    "import ipywidgets as widgets\n",
    "import json\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.callbacks import LlamaDebugHandler\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever\n",
    "from llama_index.core.indices.property_graph import (\n",
    "    SimpleLLMPathExtractor,\n",
    "    SchemaLLMPathExtractor,\n",
    "    DynamicLLMPathExtractor,\n",
    ")\n",
    "import yaml\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e9f925-0bd9-406c-a41d-8be268260b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    load_index_from_storage,\n",
    "    load_indices_from_storage,\n",
    "    load_graph_from_storage,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8291822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17dd9f9d-7e21-49ef-9a01-d8b0dd7fdc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9954a1",
   "metadata": {},
   "source": [
    "# Set LLM (OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a66b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add api key to llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f9e96ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model=\"gpt-4o\", max_tokens=3000)\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d7bc2",
   "metadata": {},
   "source": [
    "# Set local LLM for embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa4bc549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "# bge-base embedding model\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "#Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4b82",
   "metadata": {},
   "source": [
    "# Set LLM for chat  (Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d46a1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = Ollama(model=\"tinyllama\", request_timeout=120.0)\n",
    "#Settings.llm = llm\n",
    "#Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc9d70",
   "metadata": {},
   "source": [
    "# Test LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e3fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d810955e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 401 Unauthorized\"\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************mY4A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     ChatMessage(\n\u001b[1;32m      3\u001b[0m         role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a data governance consultant\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     ),\n\u001b[1;32m      5\u001b[0m     ChatMessage(role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms your favorite data tool ?\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m resp \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mchat(messages)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(resp)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_index/core/instrumentation/dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_index/core/llms/callbacks.py:173\u001b[0m, in \u001b[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001b[0;34m(_self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m event_id \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_event_start(\n\u001b[1;32m    165\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    166\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m     },\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     f_return_val \u001b[38;5;241m=\u001b[39m f(_self, messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    175\u001b[0m     callback_manager\u001b[38;5;241m.\u001b[39mon_event_end(\n\u001b[1;32m    176\u001b[0m         CBEventType\u001b[38;5;241m.\u001b[39mLLM,\n\u001b[1;32m    177\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mEXCEPTION: e},\n\u001b[1;32m    178\u001b[0m         event_id\u001b[38;5;241m=\u001b[39mevent_id,\n\u001b[1;32m    179\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_index/llms/openai/base.py:355\u001b[0m, in \u001b[0;36mOpenAI.chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     chat_fn \u001b[38;5;241m=\u001b[39m completion_to_chat_decorator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_complete)\n\u001b[0;32m--> 355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_fn(messages, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_index/llms/openai/base.py:106\u001b[0m, in \u001b[0;36mllm_retry_decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m retry \u001b[38;5;241m=\u001b[39m create_retry_decorator(\n\u001b[1;32m    100\u001b[0m     max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    101\u001b[0m     random_exponential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m     max_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    105\u001b[0m )\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retry(f)(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter(retry_state\u001b[38;5;241m=\u001b[39mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m fut\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fut\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_index/llms/openai/base.py:432\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m to_openai_message_dicts(\n\u001b[1;32m    427\u001b[0m     messages,\n\u001b[1;32m    428\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    429\u001b[0m )\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreuse_client:\n\u001b[0;32m--> 432\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    433\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessage_dicts,\n\u001b[1;32m    434\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_kwargs(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m client:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    860\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    861\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    862\u001b[0m             {\n\u001b[1;32m    863\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    864\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    865\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    866\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    867\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    868\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    869\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    870\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    871\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    872\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    873\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    874\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    875\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    876\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    877\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m    878\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    879\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m    880\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    881\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    882\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    883\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    884\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    885\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    886\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    887\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    888\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    889\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    890\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    891\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    892\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    893\u001b[0m             },\n\u001b[1;32m    894\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    895\u001b[0m         ),\n\u001b[1;32m    896\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    897\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    898\u001b[0m         ),\n\u001b[1;32m    899\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    900\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    902\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    958\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    960\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    961\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    962\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    963\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-********************************************************************************************************************************************************mY4A. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    ChatMessage(\n",
    "        role=\"system\", content=\"You are a data governance consultant\"\n",
    "    ),\n",
    "    ChatMessage(role=\"user\", content=\"What's your favorite data tool ?\"),\n",
    "]\n",
    "resp = llm.chat(messages)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47252b4e-eb58-453c-9e7d-bab5286ee296",
   "metadata": {},
   "source": [
    "# Load storage contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e8019-e150-4a60-b59d-cac7a4aebcba",
   "metadata": {},
   "source": [
    "## Load vector storage context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42c55a04-2b21-4c05-96fb-f0f9ce26bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"vector\"),\n",
    "    vector_store=SimpleVectorStore.from_persist_dir(\n",
    "        persist_dir=\"vector\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"vector\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b951a-9776-46da-9309-142acfc3f1dd",
   "metadata": {},
   "source": [
    "## Load knowledge graph storage context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a086ef48-3be3-4a12-b158-4ed50aa2b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"knowledge_graph\"),\n",
    "    graph_store=SimpleGraphStore.from_persist_dir(\n",
    "        persist_dir=\"knowledge_graph\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"knowledge_graph\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28317dfb",
   "metadata": {},
   "source": [
    "## Load onto graph storage context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db73d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_storage_context = StorageContext.from_defaults(\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=\"onto_graph\"),\n",
    "    graph_store=SimpleGraphStore.from_persist_dir(\n",
    "        persist_dir=\"onto_graph\"\n",
    "    ),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=\"onto_graph\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d278b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StorageContext(docstore=<llama_index.core.storage.docstore.simple_docstore.SimpleDocumentStore object at 0x000001DB33F79400>, index_store=<llama_index.core.storage.index_store.simple_index_store.SimpleIndexStore object at 0x000001DB33F8D910>, vector_stores={'default': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={})), 'image': SimpleVectorStore(stores_text=False, is_embedding_query=True, data=SimpleVectorStoreData(embedding_dict={}, text_id_to_ref_doc_id={}, metadata_dict={}))}, graph_store=<llama_index.core.graph_stores.simple.SimpleGraphStore object at 0x000001DB33F8DD00>, property_graph_store=None)\n"
     ]
    }
   ],
   "source": [
    "print(onto_storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c3f5b-f0fa-4e10-85fd-9ff7642de428",
   "metadata": {},
   "source": [
    "# Load index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3e85a-8fca-4138-9ff9-fca4637a47e2",
   "metadata": {},
   "source": [
    "## Load vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68274006-61a7-42d7-9556-6ddae87d2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "simple_index = load_index_from_storage(vector_storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cdab09-2c63-4e23-9ca3-79102db74cf0",
   "metadata": {},
   "source": [
    "### Set retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a6c3e925-229f-4636-a7b7-73687a3aa7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_query_engine = simple_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b32a3",
   "metadata": {},
   "source": [
    "### Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "887f57e4-8bc9-440b-a84c-0080cbd412e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863445ebffe341088988f6eecdd21819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "simple_rag_retriever = simple_index.as_retriever(\n",
    "    retriever_mode=\"hybrid\",  # or \"embedding\" or \"hybrid\"\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = simple_query_engine.query(\n",
    "    \"Quelle méthode utiliser pour prédire si un client va faire défaut sur son prêt bancaire. Fais moi un plan.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f014fd60-81a4-4b7f-aef9-f545402e0fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Pour prédire si un client va faire défaut sur son prêt bancaire, une approche structurée peut être suivie. Voici un plan détaillé :\n",
       "\n",
       "1. **Compréhension du problème :**\n",
       "   - Définir clairement l'objectif de la prédiction.\n",
       "   - Identifier les variables cibles et explicatives.\n",
       "\n",
       "2. **Collecte et préparation des données :**\n",
       "   - Rassembler les données historiques des clients, y compris les informations démographiques, financières, et comportementales.\n",
       "   - Nettoyer les données pour gérer les valeurs manquantes et les anomalies.\n",
       "   - Effectuer une analyse exploratoire des données pour comprendre les distributions et les relations entre les variables.\n",
       "\n",
       "3. **Sélection de la méthode de modélisation :**\n",
       "   - Choisir une méthode de classification, car le problème est de nature binaire (défaut ou non).\n",
       "   - Considérer des algorithmes comme la régression logistique, les arbres de décision, les forêts aléatoires, ou les machines à vecteurs de support.\n",
       "\n",
       "4. **Construction du modèle :**\n",
       "   - Diviser les données en ensembles d'entraînement et de test.\n",
       "   - Entraîner le modèle choisi sur l'ensemble d'entraînement.\n",
       "   - Optimiser les hyperparamètres pour améliorer la performance du modèle.\n",
       "\n",
       "5. **Évaluation du modèle :**\n",
       "   - Utiliser des métriques de performance telles que l'accuracy, le F1-score, la précision, et le rappel pour évaluer le modèle.\n",
       "   - Effectuer une validation croisée pour assurer la robustesse du modèle.\n",
       "\n",
       "6. **Interprétation et déploiement :**\n",
       "   - Analyser les résultats pour comprendre les facteurs influençant le défaut.\n",
       "   - Déployer le modèle dans un environnement de production pour des prédictions en temps réel.\n",
       "\n",
       "7. **Surveillance et mise à jour :**\n",
       "   - Surveiller la performance du modèle au fil du temps.\n",
       "   - Mettre à jour le modèle régulièrement avec de nouvelles données pour maintenir sa précision.\n",
       "\n",
       "Ce plan fournit une approche systématique pour prédire le défaut de paiement d'un client sur un prêt bancaire.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242ad8d-20fb-4126-9380-4d51c7e3fa32",
   "metadata": {},
   "source": [
    "## Load graph index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75e3c260-c349-43f0-b54c-6dac7a12586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "graph_index = load_index_from_storage(graph_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81c1e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = graph_index.get_networkx_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8424ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the knowledge graph: 6\n"
     ]
    }
   ],
   "source": [
    "# Count the number of nodes\n",
    "num_nodes = len(nx_graph.edges())\n",
    "\n",
    "print(f\"Number of nodes in the knowledge graph: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0388bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 9 nodes and 6 edges\n",
      "Nodes: ['Model', 'Parent', 'Type_model', 'Classification', 'Clustering', 'Regression', 'Prediction', 'Type_algo', 'Optimization algorithm']\n",
      "Edges: [('Model', 'Parent'), ('Type_model', 'Classification'), ('Type_model', 'Clustering'), ('Type_model', 'Regression'), ('Type_model', 'Prediction'), ('Type_algo', 'Optimization algorithm')]\n"
     ]
    }
   ],
   "source": [
    "g = graph_index.get_networkx_graph()\n",
    "print(g)\n",
    "print(\"Nodes:\", g.nodes())\n",
    "print(\"Edges:\", g.edges())\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "\n",
    "with open(\"knowledge_graph.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(net.generate_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d6f30",
   "metadata": {},
   "source": [
    "### Set retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "330e38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_query_engine = graph_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e323b",
   "metadata": {},
   "source": [
    "### Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0dd0ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e87f96b3d404de890ec82a6a9e6660e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 586681f4-8f47-4aef-a419-98adfbf8329d: ---\n",
      "type_algo: \"[[Optimization algorithm]]\"\n",
      "---\n",
      "> Querying with idx: 586681f4-8f47-4aef-a419-98adfbf8329d: ---\n",
      "type_algo: \"[[Optimization algorithm]]\"\n",
      "---\n",
      "> Querying with idx: 586681f4-8f47-4aef-a419-98adfbf8329d: ---\n",
      "type_algo: \"[[Optimization algorithm]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 2f7adc91-9550-4d75-9e56-13cf02007eb0: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 2f7adc91-9550-4d75-9e56-13cf02007eb0: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 2f7adc91-9550-4d75-9e56-13cf02007eb0: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: be73ac4e-23a7-47c8-baf1-799c46e41acb: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: be73ac4e-23a7-47c8-baf1-799c46e41acb: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: be73ac4e-23a7-47c8-baf1-799c46e41acb: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 35ade69a-6631-474e-9bc8-c8edfe0bfd5a: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 35ade69a-6631-474e-9bc8-c8edfe0bfd5a: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 35ade69a-6631-474e-9bc8-c8edfe0bfd5a: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "graph_rag_retriever = graph_index.as_retriever(\n",
    "    retriever_mode=\"hybrid\",  # or \"embedding\" or \"hybrid\"\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = graph_query_engine.query(\n",
    "    \"Quelle méthode utiliser pour prédire si un client va faire défaut sur son prêt bancaire. Fais moi un plan.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d1031a44-f06e-400f-beaa-2065beeee9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Pour prédire si un client va faire défaut sur son prêt bancaire, une méthode de classification serait appropriée. Voici un plan pour aborder ce problème :\n",
       "\n",
       "1. **Collecte des données**: Rassemblez des données historiques sur les clients, y compris des informations démographiques, des antécédents de crédit, des revenus, etc.\n",
       "\n",
       "2. **Prétraitement des données**: Nettoyez les données en traitant les valeurs manquantes, en normalisant les variables et en encodant les variables catégorielles.\n",
       "\n",
       "3. **Sélection des caractéristiques**: Identifiez les caractéristiques les plus pertinentes qui influencent le défaut de paiement à l'aide de techniques de sélection de caractéristiques.\n",
       "\n",
       "4. **Choix du modèle**: Utilisez un modèle de classification, tel que la régression logistique, les arbres de décision, ou les forêts aléatoires, pour prédire le défaut de paiement.\n",
       "\n",
       "5. **Entraînement du modèle**: Divisez les données en ensembles d'entraînement et de test, puis entraînez le modèle sur l'ensemble d'entraînement.\n",
       "\n",
       "6. **Évaluation du modèle**: Évaluez la performance du modèle à l'aide de l'ensemble de test en utilisant des métriques telles que la précision, le rappel, et la courbe ROC.\n",
       "\n",
       "7. **Optimisation**: Affinez le modèle en ajustant les hyperparamètres pour améliorer sa performance.\n",
       "\n",
       "8. **Déploiement**: Intégrez le modèle dans le système de décision pour prédire le risque de défaut des nouveaux clients.\n",
       "\n",
       "9. **Surveillance et mise à jour**: Surveillez la performance du modèle et mettez-le à jour régulièrement avec de nouvelles données pour maintenir sa précision.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1e7541",
   "metadata": {},
   "source": [
    "## Load onto index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb203e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.loading:Loading all indices.\n",
      "Loading all indices.\n",
      "Loading all indices.\n"
     ]
    }
   ],
   "source": [
    "onto_storage_context = StorageContext.from_defaults(persist_dir=\"onto_graph\")\n",
    "# Load the PropertyGraphIndex from the storage context\n",
    "\n",
    "onto_index = load_index_from_storage(onto_storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a182d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_index.property_graph_store.save_networkx_graph(\n",
    "    name=\"OntoGraph.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4dcae",
   "metadata": {},
   "source": [
    "### Set retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bee17732",
   "metadata": {},
   "outputs": [],
   "source": [
    "onto_query_engine = onto_index.as_query_engine(\n",
    " include_text=True,\n",
    " similarity_top_k=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95920ba",
   "metadata": {},
   "source": [
    "### Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c400086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9763131b78a448a8bb71c69f6f879ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "onto_rag_retriever = onto_index.as_retriever(\n",
    "    retriever_mode=\"hybrid\",  # or \"embedding\" or \"hybrid\"\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = onto_query_engine.query(\n",
    "    \"Quelle méthode utiliser pour prédire si un client va faire défaut sur son prêt bancaire. Fais moi un plan.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "123366e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Pour prédire si un client va faire défaut sur son prêt bancaire, il est approprié d'utiliser une méthode de classification. Voici un plan pour aborder ce problème :\n",
       "\n",
       "1. **Compréhension du problème :**\n",
       "   - Définir clairement l'objectif : prédire le défaut de paiement.\n",
       "   - Identifier les variables cibles et explicatives.\n",
       "\n",
       "2. **Collecte et préparation des données :**\n",
       "   - Rassembler les données historiques des clients, y compris les caractéristiques démographiques, financières, et comportementales.\n",
       "   - Nettoyer les données pour gérer les valeurs manquantes et les anomalies.\n",
       "\n",
       "3. **Exploration des données :**\n",
       "   - Analyser les données pour comprendre les distributions et les relations entre les variables.\n",
       "   - Utiliser des visualisations pour identifier les tendances et les corrélations.\n",
       "\n",
       "4. **Sélection des caractéristiques :**\n",
       "   - Choisir les variables les plus pertinentes pour la prédiction.\n",
       "   - Éventuellement, réduire la dimensionnalité avec des techniques comme l'analyse en composantes principales (PCA).\n",
       "\n",
       "5. **Choix du modèle de classification :**\n",
       "   - Considérer des modèles tels que la régression logistique, les arbres de décision, les forêts aléatoires, ou les machines à vecteurs de support (SVM).\n",
       "   - Comparer les modèles en termes de précision, de rappel, et de courbe ROC.\n",
       "\n",
       "6. **Entraînement et validation du modèle :**\n",
       "   - Diviser les données en ensembles d'entraînement et de test.\n",
       "   - Entraîner le modèle sur l'ensemble d'entraînement.\n",
       "   - Valider le modèle avec l'ensemble de test pour évaluer sa performance.\n",
       "\n",
       "7. **Optimisation du modèle :**\n",
       "   - Ajuster les hyperparamètres pour améliorer la performance.\n",
       "   - Utiliser des techniques de validation croisée pour éviter le surapprentissage.\n",
       "\n",
       "8. **Interprétation et déploiement :**\n",
       "   - Interpréter les résultats pour comprendre les facteurs influençant le défaut de paiement.\n",
       "   - Déployer le modèle dans un environnement de production pour des prédictions en temps réel.\n",
       "\n",
       "9. **Suivi et mise à jour :**\n",
       "   - Surveiller la performance du modèle au fil du temps.\n",
       "   - Mettre à jour le modèle avec de nouvelles données pour maintenir sa précision. \n",
       "\n",
       "Ce plan vous guidera dans la mise en œuvre d'une solution de classification pour prédire le défaut de paiement des clients.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b65be-955e-44db-942e-27e235384f19",
   "metadata": {},
   "source": [
    "# Visualize knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55ef03a2-2e7e-443d-865f-b6d991d16416",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = graph_index.get_networkx_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9d58e14-0017-494f-98ec-ad8b30bf8c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the knowledge graph: 6\n"
     ]
    }
   ],
   "source": [
    "# Count the number of nodes\n",
    "num_nodes = len(nx_graph.edges())\n",
    "\n",
    "print(f\"Number of nodes in the knowledge graph: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a113918c-f195-4c8c-b0f3-c85d7a6c5d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 9 nodes and 6 edges\n",
      "Nodes: ['Model', 'Parent', 'Type_model', 'Classification', 'Clustering', 'Regression', 'Prediction', 'Type_algo', 'Optimization algorithm']\n",
      "Edges: [('Model', 'Parent'), ('Type_model', 'Classification'), ('Type_model', 'Clustering'), ('Type_model', 'Regression'), ('Type_model', 'Prediction'), ('Type_algo', 'Optimization algorithm')]\n"
     ]
    }
   ],
   "source": [
    "g = graph_index.get_networkx_graph()\n",
    "print(g)\n",
    "print(\"Nodes:\", g.nodes())\n",
    "print(\"Edges:\", g.edges())\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "\n",
    "with open(\"knowledge_graph.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(net.generate_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab94fc0-2080-4869-94a0-b013dc0fbd9d",
   "metadata": {},
   "source": [
    "# (Simple) Query the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c93bdbb-aaa4-4658-aecf-1cd7b5038ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd9891258824d83b2717948d2ab876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Quelle méthode utiliser pour prédire si un client va faire défaut sur son prêt bancaire. Fais moi un plan.\"\n",
    "query_engine = simple_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")\n",
    "\n",
    "response = query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e2a871b4-70c8-45ed-a8b9-b2aaa8d15a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Pour prédire si un client va faire défaut sur son prêt bancaire, une approche structurée peut être suivie. Voici un plan détaillé :\n",
       "\n",
       "1. **Compréhension du problème :**\n",
       "   - Définir clairement l'objectif de la prédiction.\n",
       "   - Identifier les variables cibles et explicatives.\n",
       "\n",
       "2. **Collecte et préparation des données :**\n",
       "   - Rassembler les données historiques des clients, y compris les informations démographiques, financières, et comportementales.\n",
       "   - Nettoyer les données pour gérer les valeurs manquantes et les anomalies.\n",
       "   - Effectuer une analyse exploratoire des données pour comprendre les distributions et les relations.\n",
       "\n",
       "3. **Choix de la méthode de modélisation :**\n",
       "   - Utiliser une méthode de classification, car le problème est de nature binaire (défaut ou non).\n",
       "   - Considérer des algorithmes comme la régression logistique, les arbres de décision, ou les forêts aléatoires.\n",
       "\n",
       "4. **Construction du modèle :**\n",
       "   - Diviser les données en ensembles d'entraînement et de test.\n",
       "   - Entraîner le modèle choisi sur l'ensemble d'entraînement.\n",
       "   - Ajuster les hyperparamètres pour optimiser les performances.\n",
       "\n",
       "5. **Évaluation du modèle :**\n",
       "   - Utiliser des métriques telles que l'accuracy, le rappel, la précision, et l'AUC-ROC pour évaluer la performance du modèle.\n",
       "   - Comparer les performances de différents modèles pour choisir le meilleur.\n",
       "\n",
       "6. **Interprétation et déploiement :**\n",
       "   - Interpréter les résultats pour comprendre les facteurs influençant le défaut.\n",
       "   - Déployer le modèle dans un environnement de production pour des prédictions en temps réel.\n",
       "\n",
       "7. **Suivi et mise à jour :**\n",
       "   - Surveiller la performance du modèle au fil du temps.\n",
       "   - Mettre à jour le modèle avec de nouvelles données pour maintenir sa précision.\n",
       "\n",
       "Ce plan permet de structurer le processus de prédiction de manière efficace et rigoureuse.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099031c-b137-468f-b20c-9c6fab1c4eb1",
   "metadata": {},
   "source": [
    "# (Simple) Query the knowledge graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8948dd2b-acd4-4621-a2d1-b45034a21007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35233b21f8b4fdea50bdbb1ea55bb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 586681f4-8f47-4aef-a419-98adfbf8329d: ---\n",
      "type_algo: \"[[Optimization algorithm]]\"\n",
      "---\n",
      "> Querying with idx: 586681f4-8f47-4aef-a419-98adfbf8329d: ---\n",
      "type_algo: \"[[Optimization algorithm]]\"\n",
      "---\n",
      "> Querying with idx: 586681f4-8f47-4aef-a419-98adfbf8329d: ---\n",
      "type_algo: \"[[Optimization algorithm]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 2f7adc91-9550-4d75-9e56-13cf02007eb0: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 2f7adc91-9550-4d75-9e56-13cf02007eb0: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 2f7adc91-9550-4d75-9e56-13cf02007eb0: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: be73ac4e-23a7-47c8-baf1-799c46e41acb: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: be73ac4e-23a7-47c8-baf1-799c46e41acb: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: be73ac4e-23a7-47c8-baf1-799c46e41acb: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 35ade69a-6631-474e-9bc8-c8edfe0bfd5a: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 35ade69a-6631-474e-9bc8-c8edfe0bfd5a: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "> Querying with idx: 35ade69a-6631-474e-9bc8-c8edfe0bfd5a: ---\n",
      "parent: \"[[Model]]\"\n",
      "---\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Quelle méthode utiliser pour prédire si un client va faire défaut sur son prêt bancaire. Fais moi un plan\"\n",
    "graph_query_engine = graph_index.as_query_engine(\n",
    " include_text=True,\n",
    " response_mode=\"tree_summarize\",\n",
    " embedding_mode=\"hybrid\",\n",
    " similarity_top_k=8,\n",
    ")\n",
    "\n",
    "response = graph_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ff5382fd-1fc8-4faf-9356-c9bb3e478db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Pour prédire si un client va faire défaut sur son prêt bancaire, une méthode de classification est appropriée. Voici un plan pour aborder ce problème :\n",
       "\n",
       "1. **Collecte des données**: Rassemblez des données historiques sur les clients, y compris des informations démographiques, des antécédents de crédit, des revenus, etc.\n",
       "\n",
       "2. **Prétraitement des données**: Nettoyez les données en traitant les valeurs manquantes, en normalisant les variables et en encodant les variables catégorielles.\n",
       "\n",
       "3. **Sélection des caractéristiques**: Identifiez les caractéristiques les plus pertinentes qui influencent la probabilité de défaut.\n",
       "\n",
       "4. **Choix du modèle**: Utilisez un modèle de classification, tel que la régression logistique, les arbres de décision, ou les forêts aléatoires, pour prédire le défaut.\n",
       "\n",
       "5. **Entraînement du modèle**: Divisez les données en ensembles d'entraînement et de test, puis entraînez le modèle sur l'ensemble d'entraînement.\n",
       "\n",
       "6. **Évaluation du modèle**: Évaluez la performance du modèle à l'aide de l'ensemble de test en utilisant des métriques telles que la précision, le rappel, et la courbe ROC.\n",
       "\n",
       "7. **Optimisation**: Ajustez les hyperparamètres du modèle pour améliorer sa performance.\n",
       "\n",
       "8. **Déploiement**: Intégrez le modèle dans le système de décision pour prédire le risque de défaut des nouveaux clients.\n",
       "\n",
       "9. **Surveillance et mise à jour**: Surveillez la performance du modèle et mettez-le à jour régulièrement avec de nouvelles données pour maintenir sa précision.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef119d0",
   "metadata": {},
   "source": [
    "# (Simple) Query the onto graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a8db6420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b298f52a587649a9bf84ec49ea03f985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Quelle méthode utiliser pour prédire si un client va faire défaut sur son prêt bancaire. Fais moi un plan.\"\n",
    "onto_query_engine = onto_index.as_query_engine(\n",
    " include_text=True,\n",
    " similarity_top_k=2,\n",
    ")\n",
    "\n",
    "response = onto_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "adcd4265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Pour prédire si un client va faire défaut sur son prêt bancaire, il est approprié d'utiliser une méthode de classification. Voici un plan pour aborder ce problème :\n",
       "\n",
       "1. **Collecte des données**: Rassembler des données historiques sur les clients, y compris des informations démographiques, financières, et des antécédents de crédit.\n",
       "\n",
       "2. **Prétraitement des données**:\n",
       "   - Nettoyage des données pour gérer les valeurs manquantes et les anomalies.\n",
       "   - Normalisation ou standardisation des variables si nécessaire.\n",
       "   - Encodage des variables catégorielles.\n",
       "\n",
       "3. **Séparation des données**:\n",
       "   - Diviser les données en ensembles d'entraînement et de test pour évaluer la performance du modèle.\n",
       "\n",
       "4. **Choix du modèle de classification**:\n",
       "   - Sélectionner un ou plusieurs algorithmes de classification, tels que la régression logistique, les arbres de décision, ou les forêts aléatoires.\n",
       "\n",
       "5. **Entraînement du modèle**:\n",
       "   - Utiliser l'ensemble d'entraînement pour ajuster le modèle choisi.\n",
       "\n",
       "6. **Évaluation du modèle**:\n",
       "   - Tester le modèle sur l'ensemble de test.\n",
       "   - Utiliser des métriques telles que la précision, le rappel, et la courbe ROC pour évaluer la performance.\n",
       "\n",
       "7. **Optimisation**:\n",
       "   - Ajuster les hyperparamètres pour améliorer la performance du modèle.\n",
       "   - Envisager l'utilisation de techniques d'ensemble pour combiner plusieurs modèles.\n",
       "\n",
       "8. **Déploiement**:\n",
       "   - Mettre en place le modèle dans un environnement de production pour prédire les défauts de paiement des nouveaux clients.\n",
       "\n",
       "9. **Surveillance et mise à jour**:\n",
       "   - Surveiller la performance du modèle au fil du temps et le mettre à jour avec de nouvelles données pour maintenir sa précision.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582274ba",
   "metadata": {},
   "source": [
    "## (Node retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "16db2a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4498fc6495420cb12ee7ba30b686f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classification]]\"\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "retriever = onto_index.as_retriever(\n",
    "    include_text=True,  # include source text, default True\n",
    ")\n",
    "\n",
    "nodes = retriever.retrieve(\"Quelle méthode utiliser pour prédire si un client va faire défaut sur son prêt bancaire ?\")\n",
    "\n",
    "\n",
    "for node in nodes:\n",
    "    print(node.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a895520-d196-476f-b569-69a67484c120",
   "metadata": {},
   "source": [
    "# Have a real chat with your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3764d-a084-4f36-9d8c-3613aa9fd835",
   "metadata": {},
   "source": [
    "## Set up the engines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e55a2-a9a9-4c4e-89a4-c4dfba55ebe4",
   "metadata": {},
   "source": [
    "### Vector engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1581c90c-cf5a-40d3-81b0-086238068f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "vector_chat_engine = simple_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \" \"\n",
    "        \" \"\n",
    "        \".\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a1c22-111b-4793-8a3d-bd3eae7e3ec7",
   "metadata": {},
   "source": [
    "### Graph engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "98b73cc2-0588-4b96-9aa2-1dfaf4cadb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "graph_chat_engine = graph_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6897ec22",
   "metadata": {},
   "source": [
    "### Onto engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54d04e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#onto_chat_engine = onto_index.query_engine(\n",
    "#    chat_mode=\"condense_plus_context\",\n",
    "#    llm=llm\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e0a08-f6ff-4ee6-a165-9b9fd90621fa",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "12804e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "709a37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "graph_chat_engine = graph_index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \" \"\n",
    "        \" \"\n",
    "        \" \"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "da3e3ebf-18dc-4bdc-87dc-58d0227e9c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.chat_engine.condense_plus_context:Condensed question: \n",
      "Condensed question: \n",
      "Condensed question: \n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd767c6d50ff4e2ba6651c6dbb8ff1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "response_stream = graph_chat_engine.stream_chat(\"\"\"\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "91ebf21f-1990-4a4d-9c9a-868872f03042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "generate = response_stream.print_response_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5a056a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>None</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{generate}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6660a81e-ad67-4d44-a86d-0ad7b77f626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_modele = \"\"\" \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c0e462e-d3ee-463a-8d49-f09dc2396b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:llama_index.core.chat_engine.condense_plus_context:Condensed question: Pouvez-vous imiter le style d'écriture de la section modèle ?\n",
      "Condensed question: Pouvez-vous imiter le style d'écriture de la section modèle ?\n",
      "Condensed question: Pouvez-vous imiter le style d'écriture de la section modèle ?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e510171c88a477dad64ac486187b275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "> Querying with idx: 4a3c46d4-923d-473c-9f33-b7c104a81217: ---\n",
      "type_model:\n",
      "  - \"[[Clustering]]\"\n",
      "  - \"[[Regression]]\"\n",
      "  - \"[[Classificati...\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "> Querying with idx: 34355420-d17b-4534-b492-ade4a9ad7720: ---\n",
      "type_model:\n",
      "  - \"[[Classification]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "> Querying with idx: 20e84990-8d4c-41ad-a31d-62a63edd8a65: ---\n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "usage: \"[[Prediction]]\"\n",
      "---\n",
      "INFO:llama_index.core.indices.knowledge_graph.retrievers:> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n",
      "> Querying with idx: 7ec05980-7510-4ca8-bb6c-7a4d13b52215: ---\n",
      "used by: \n",
      "type_model:\n",
      "  - \"[[Regression]]\"\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "response_stream = graph_chat_engine.stream_chat(\"\"\"Imite le style d'écriture de la {section_modele}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c86075ee-12c8-4b97-8b51-0c717fc4b1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Je suis désolé, mais il semble qu'il y ait eu une petite confusion dans votre demande. Pourriez-vous préciser quel style d'écriture ou quel modèle vous aimeriez que j'imite ? Cela m'aiderait à mieux répondre à votre demande. Merci !"
     ]
    }
   ],
   "source": [
    "generate = response_stream.print_response_stream()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ffb78d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>None</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"<b>{generate}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f74a302-bb28-4bf8-9103-1f62750a5fc0",
   "metadata": {},
   "source": [
    "## Sum-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d81f7480-9d62-4952-948e-19e089d9da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = memory.get_all()\n",
    "\n",
    "# Assuming chat_history is available and contains your messages\n",
    "assistant_messages = [\n",
    "    message.content \n",
    "    for message in chat_history \n",
    "    if message.role == MessageRole.ASSISTANT  # Compare with the enum directly\n",
    "]\n",
    "\n",
    "\n",
    "output_filename = r\"C:\\Users\\asarazin\\OneDrive - Veltys Max\\Documents\\documente-marcel\\output\\output.md\"\n",
    "# Write to a Markdown file\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for msg in assistant_messages:\n",
    "        f.write(msg + \"\\n\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
